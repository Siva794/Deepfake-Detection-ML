{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6963d24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: C:\\Users\\ksiva\\Desktop\\Deepfake-Detection\\dataset\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 0.6932\n",
      "Batch 10, Loss: 0.6339\n",
      "Batch 20, Loss: 0.4599\n",
      "Batch 30, Loss: 0.1215\n",
      "Batch 40, Loss: 0.0553\n",
      "Batch 50, Loss: 0.5108\n",
      "Batch 60, Loss: 0.1600\n",
      "Batch 70, Loss: 0.0548\n",
      "Batch 80, Loss: 0.0904\n",
      "Batch 90, Loss: 0.0680\n",
      "Batch 100, Loss: 0.1000\n",
      "Batch 110, Loss: 0.2523\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 193\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m train_loss, train_acc = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m train_losses.append(train_loss)\n\u001b[32m    195\u001b[39m train_accuracies.append(train_acc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    135\u001b[39m output = model(data)\n\u001b[32m    136\u001b[39m loss = criterion(output, target)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m optimizer.step()\n\u001b[32m    140\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ksiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ksiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ksiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# train_vit.py\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"USE_TORCH\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "DATA_DIR = r\"C:\\Users\\ksiva\\Desktop\\Deepfake-Detection\\dataset\"  # path to your dataset folder\n",
    "OUTPUT_DIR = \"./vit-deepfake-model\"\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "# =============================\n",
    "\n",
    "# Custom Dataset Class\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, split='train'):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "        if split == 'train':\n",
    "            # For training, use the train directory\n",
    "            self.fake_images = glob.glob(os.path.join(data_dir, 'train', 'Fake', '*.jpg'))\n",
    "            # We need real images for training too, let's use some from test\n",
    "            self.real_images = glob.glob(os.path.join(data_dir, 'test', 'real', '*.jpg'))\n",
    "            # Take only a subset of real images to balance the dataset\n",
    "            self.real_images = self.real_images[:len(self.fake_images)]\n",
    "        else:\n",
    "            # For testing, use the test directory\n",
    "            self.fake_images = glob.glob(os.path.join(data_dir, 'test', 'fake', '*.jpg'))\n",
    "            self.real_images = glob.glob(os.path.join(data_dir, 'test', 'real', '*.jpg'))\n",
    "        \n",
    "        # Combine and shuffle\n",
    "        self.images = self.real_images + self.fake_images\n",
    "        self.labels = [0] * len(self.real_images) + [1] * len(self.fake_images)\n",
    "        \n",
    "        # Shuffle data\n",
    "        combined = list(zip(self.images, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        self.images, self.labels = zip(*combined)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Simple CNN Model (since ViT has dependency issues)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading dataset from:\", DATA_DIR)\n",
    "train_dataset = DeepfakeDataset(DATA_DIR, transform=transform, split='train')\n",
    "test_dataset = DeepfakeDataset(DATA_DIR, transform=transform, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleCNN(num_classes=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training function\n",
    "def train_model():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total += target.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            all_preds.extend(pred.cpu().numpy().flatten())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\")\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_model()\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_acc, precision, recall, f1 = evaluate_model()\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# Save model\n",
    "print(\"\\nSaving model to:\", OUTPUT_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, 'model.pth'))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, EPOCHS+1), train_losses, 'b-', label='Train Loss')\n",
    "plt.plot(range(1, EPOCHS+1), val_losses, 'r-', label='Val Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, EPOCHS+1), train_accuracies, 'b-', label='Train Accuracy')\n",
    "plt.plot(range(1, EPOCHS+1), val_accuracies, 'r-', label='Val Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results_accuracy_curve.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "print(f\"Final Validation F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5827d72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: C:\\Users\\ksiva\\Desktop\\Deepfake-Detection\\dataset\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1 Summary ---\n",
      "Train Loss: 0.2282, Train Acc: 92.94%\n",
      "Val Loss: 1.2299, Val Acc: 51.62%\n",
      "Precision: 0.5102, Recall: 0.9802, F1: 0.6711\n",
      "Time for this epoch: 1:44:34\n",
      "Elapsed: 1:44:34, Estimated time left: 6:58:17\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 2 Summary ---\n",
      "Train Loss: 0.1964, Train Acc: 93.55%\n",
      "Val Loss: 0.9526, Val Acc: 58.08%\n",
      "Precision: 0.5621, Recall: 0.7595, F1: 0.6460\n",
      "Time for this epoch: 1:57:03\n",
      "Elapsed: 3:41:37, Estimated time left: 5:32:25\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 3 Summary ---\n",
      "Train Loss: 0.1695, Train Acc: 94.19%\n",
      "Val Loss: 0.9613, Val Acc: 60.91%\n",
      "Precision: 0.5780, Recall: 0.8288, F1: 0.6811\n",
      "Time for this epoch: 0:36:03\n",
      "Elapsed: 4:17:40, Estimated time left: 2:51:47\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 4 Summary ---\n",
      "Train Loss: 0.1543, Train Acc: 94.62%\n",
      "Val Loss: 0.9210, Val Acc: 63.62%\n",
      "Precision: 0.6000, Recall: 0.8328, F1: 0.6975\n",
      "Time for this epoch: 1:44:59\n",
      "Elapsed: 6:02:39, Estimated time left: 1:30:39\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 5 Summary ---\n",
      "Train Loss: 0.1402, Train Acc: 94.99%\n",
      "Val Loss: 0.7210, Val Acc: 67.83%\n",
      "Precision: 0.6570, Recall: 0.7558, F1: 0.7030\n",
      "Time for this epoch: 1:04:18\n",
      "Elapsed: 7:06:58, Estimated time left: 0:00:00\n",
      "\n",
      "Saving model to: ./vit-deepfake-model\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "Final Validation Accuracy: 67.83%\n",
      "Final Validation F1 Score: 0.7030\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"USE_TORCH\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "DATA_DIR = r\"C:\\Users\\ksiva\\Desktop\\Deepfake-Detection\\dataset\"  # path to your dataset folder\n",
    "OUTPUT_DIR = \"./vit-deepfake-model\"\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "# =============================\n",
    "\n",
    "# Custom Dataset Class\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, split='train'):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.fake_images = glob.glob(os.path.join(data_dir, 'train', 'Fake', '*.jpg'))\n",
    "            self.real_images = glob.glob(os.path.join(data_dir, 'test', 'real', '*.jpg'))\n",
    "            self.real_images = self.real_images[:len(self.fake_images)]\n",
    "        else:\n",
    "            self.fake_images = glob.glob(os.path.join(data_dir, 'test', 'fake', '*.jpg'))\n",
    "            self.real_images = glob.glob(os.path.join(data_dir, 'test', 'real', '*.jpg'))\n",
    "        \n",
    "        self.images = self.real_images + self.fake_images\n",
    "        self.labels = [0] * len(self.real_images) + [1] * len(self.fake_images)\n",
    "        \n",
    "        combined = list(zip(self.images, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        self.images, self.labels = zip(*combined)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Simple CNN Model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading dataset from:\", DATA_DIR)\n",
    "train_dataset = DeepfakeDataset(DATA_DIR, transform=transform, split='train')\n",
    "test_dataset = DeepfakeDataset(DATA_DIR, transform=transform, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleCNN(num_classes=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training function with tqdm progress bar\n",
    "def train_model():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for data, target in progress_bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "            \n",
    "            all_preds.extend(pred.cpu().numpy().flatten())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\")\n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "# Training loop with ETA\n",
    "print(\"\\nStarting training...\")\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    train_loss, train_acc = train_model()\n",
    "    val_loss, val_acc, precision, recall, f1 = evaluate_model()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    epoch_duration = epoch_end - epoch_start\n",
    "    elapsed = epoch_end - start_time\n",
    "    avg_epoch_time = elapsed / (epoch + 1)\n",
    "    remaining_time = avg_epoch_time * (EPOCHS - (epoch + 1))\n",
    "\n",
    "    print(f\"\\n--- Epoch {epoch+1} Summary ---\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    print(f\"Time for this epoch: {timedelta(seconds=int(epoch_duration))}\")\n",
    "    print(f\"Elapsed: {timedelta(seconds=int(elapsed))}, Estimated time left: {timedelta(seconds=int(remaining_time))}\")\n",
    "\n",
    "# Save model\n",
    "print(\"\\nSaving model to:\", OUTPUT_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, 'model.pth'))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, EPOCHS+1), train_losses, 'b-', label='Train Loss')\n",
    "plt.plot(range(1, EPOCHS+1), val_losses, 'r-', label='Val Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, EPOCHS+1), train_accuracies, 'b-', label='Train Accuracy')\n",
    "plt.plot(range(1, EPOCHS+1), val_accuracies, 'r-', label='Val Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results_accuracy_curve.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "print(f\"Final Validation F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d2918f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Real Image Detected\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained PyTorch model weights\n",
    "model.load_state_dict(torch.load(\"./vit-deepfake-model/model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "img_path = \"dataset/test/real/real_8.jpg\"  # or real sample\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(img_tensor)\n",
    "    pred = torch.softmax(output, dim=1)[0][1].item()  # Probability for 'fake' class\n",
    "\n",
    "if pred > 0.5:\n",
    "    print(\" Fake Image Detected\")\n",
    "else:\n",
    "    print(\" Real Image Detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04660a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Save PyTorch model weights\n",
    "torch.save(model.state_dict(), \"vit-deepfake-model.pth\")  #  PyTorch format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe48ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading dataset from: C:\\Users\\ksiva\\Desktop\\Deepfake-Detection\\dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksiva\\AppData\\Local\\Temp\\ipykernel_19916\\1113002262.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found existing model at ./vit-deepfake-model\\model.pth. Loading...\n",
      " Resumed from epoch 8\n",
      "\n",
      " Starting continuous training (Press Ctrl + C to stop)...\n",
      "\n",
      "===== Epoch 9 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|       | 540/1751 [07:05<11:14,  1.80it/s, loss=0.1510]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"USE_TORCH\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "DATA_DIR = r\"C:\\Users\\ksiva\\Desktop\\Deepfake-Detection\\dataset\"\n",
    "OUTPUT_DIR = \"./vit-deepfake-model\"\n",
    "CHECKPOINT_PATH = os.path.join(OUTPUT_DIR, \"model.pth\")\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "# =============================\n",
    "\n",
    "# Custom Dataset Class\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, split='train'):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "\n",
    "        if split == 'train':\n",
    "            self.fake_images = glob.glob(os.path.join(data_dir, 'train', 'Fake', '*.jpg'))\n",
    "            self.real_images = glob.glob(os.path.join(data_dir, 'train', 'Real', '*.jpg'))\n",
    "        else:\n",
    "            self.fake_images = glob.glob(os.path.join(data_dir, 'test', 'fake', '*.jpg'))\n",
    "            self.real_images = glob.glob(os.path.join(data_dir, 'test', 'real', '*.jpg'))\n",
    "\n",
    "        self.images = self.real_images + self.fake_images\n",
    "        self.labels = [0] * len(self.real_images) + [1] * len(self.fake_images)\n",
    "\n",
    "        combined = list(zip(self.images, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        self.images, self.labels = zip(*combined)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Simple CNN Model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "print(\" Loading dataset from:\", DATA_DIR)\n",
    "train_dataset = DeepfakeDataset(DATA_DIR, transform=transform, split='train')\n",
    "test_dataset = DeepfakeDataset(DATA_DIR, transform=transform, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleCNN(num_classes=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Trackers\n",
    "train_losses, val_losses, val_accs = [], [], []\n",
    "start_epoch = 0\n",
    "\n",
    "#  Load checkpoint if it exists\n",
    "#  Load existing model or checkpoint\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\" Found existing model at {CHECKPOINT_PATH}. Loading...\")\n",
    "\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    \n",
    "    # Check if it's a full checkpoint (dict with keys) or just raw model weights\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        val_losses = checkpoint.get('val_losses', [])\n",
    "        val_accs = checkpoint.get('val_accs', [])\n",
    "        print(f\" Resumed from epoch {start_epoch}\")\n",
    "    else:\n",
    "        # It's just raw model weights\n",
    "        model.load_state_dict(checkpoint)\n",
    "        start_epoch = 0\n",
    "        print(\" Loaded raw model weights. Starting training from epoch 0.\")\n",
    "else:\n",
    "    print(\" No saved model found. Starting training from scratch.\")\n",
    "    start_epoch = 0\n",
    "\n",
    "\n",
    "# Training and Evaluation\n",
    "def train_model():\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for data, target in progress_bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    return total_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "            all_preds.extend(pred.cpu().numpy().flatten())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    acc = 100. * correct / total\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\")\n",
    "    return avg_loss, acc, precision, recall, f1\n",
    "\n",
    "\n",
    "#  Continuous training\n",
    "print(\"\\n Starting continuous training (Press Ctrl + C to stop)...\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "epoch = start_epoch\n",
    "try:\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        print(f\"\\n===== Epoch {epoch} =====\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train_model()\n",
    "        val_loss, val_acc, precision, recall, f1 = evaluate_model()\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        elapsed = timedelta(seconds=int(time.time() - start_time))\n",
    "        print(f\" Epoch {epoch} Summary:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
    "        print(f\" Time: {elapsed}\")\n",
    "\n",
    "        #  Save checkpoint safely\n",
    "        temp_path = CHECKPOINT_PATH + \".tmp\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accs': val_accs\n",
    "        }, temp_path)\n",
    "        os.replace(temp_path, CHECKPOINT_PATH)\n",
    "        print(f\" Checkpoint saved at epoch {epoch} -> {CHECKPOINT_PATH}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n Training stopped manually. Checkpoint saved safely.\")\n",
    "\n",
    "#  Plot after training stops\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accs, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_progress.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Training session completed. Latest checkpoint saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072d668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n",
      "True\n",
      "True\n",
      "NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)          # Should show CUDA version, e.g., '12.2'\n",
    "print(torch.backends.cudnn.enabled)  # Should be True\n",
    "print(torch.cuda.is_available())   # Should be True\n",
    "print(torch.cuda.get_device_name(0))  # Should show RTX 4050\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e459a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using device: cuda\n",
      " Max batch size 64 fits fine.\n",
      " Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksiva\\AppData\\Local\\Temp\\ipykernel_24132\\1366128903.py:149: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # mixed precision\n",
      "C:\\Users\\ksiva\\AppData\\Local\\Temp\\ipykernel_24132\\1366128903.py:156: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(CHECKPOINT_PATH, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Resumed from epoch 2\n",
      "\n",
      "===== Epoch 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2188 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"USE_TORCH\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import glob, random, time\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "DATA_DIR = r\"C:\\Users\\ksiva\\Desktop\\Deepfake-Detection\\dataset\"\n",
    "OUTPUT_DIR = \"./vit-deepfake-model\"\n",
    "CHECKPOINT_PATH = os.path.join(OUTPUT_DIR, \"model.pth\")\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_EPOCHS = 30        # stop after 30 epochs (you can change this)\n",
    "# ============================\n",
    "\n",
    "# ---------- Dataset ----------\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, split='train'):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "\n",
    "        self.fake_images = glob.glob(os.path.join(data_dir, split, 'Fake', '*.jpg'))\n",
    "        if not self.fake_images:  # handle lowercase folders\n",
    "            self.fake_images = glob.glob(os.path.join(data_dir, split, 'fake', '*.jpg'))\n",
    "\n",
    "        self.real_images = glob.glob(os.path.join(data_dir, split, 'Real', '*.jpg'))\n",
    "        if not self.real_images:\n",
    "            self.real_images = glob.glob(os.path.join(data_dir, split, 'real', '*.jpg'))\n",
    "\n",
    "        self.images = self.real_images + self.fake_images\n",
    "        self.labels = [0] * len(self.real_images) + [1] * len(self.fake_images)\n",
    "\n",
    "        combined = list(zip(self.images, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        self.images, self.labels = zip(*combined)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# ---------- Model ----------\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ---------- Transforms ----------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ---------- Device ----------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\" Using device: {device}\")\n",
    "\n",
    "# ---------- Auto Batch Size Test ----------\n",
    "def find_best_batch_size(model, device, base=16):\n",
    "    test_input = torch.randn(base, 3, 224, 224).to(device)\n",
    "    while True:\n",
    "        try:\n",
    "            _ = model(test_input)\n",
    "            base += 8\n",
    "            test_input = torch.randn(base, 3, 224, 224).to(device)\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                torch.cuda.empty_cache()\n",
    "                base -= 8\n",
    "                break\n",
    "    print(f\" Best batch size that fits: {base}\")\n",
    "    return base\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "def safe_find_best_batch_size(model, device, start=32, step=4, max_try=64):\n",
    "    batch = start\n",
    "    while batch <= max_try:\n",
    "        try:\n",
    "            x = torch.randn(batch, 3, 224, 224, device=device)\n",
    "            with torch.no_grad():\n",
    "                _ = model(x)\n",
    "            torch.cuda.empty_cache()\n",
    "            batch += step\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                torch.cuda.empty_cache()\n",
    "                batch -= step\n",
    "                print(f\" Best batch size that fits: {batch}\")\n",
    "                return batch\n",
    "            else:\n",
    "                raise\n",
    "    print(f\" Max batch size {max_try} fits fine.\")\n",
    "    return max_try\n",
    "\n",
    "BATCH_SIZE = safe_find_best_batch_size(model, device)\n",
    "\n",
    "\n",
    "# ---------- Data ----------\n",
    "print(\" Loading dataset...\")\n",
    "train_dataset = DeepfakeDataset(DATA_DIR, transform, 'train')\n",
    "test_dataset  = DeepfakeDataset(DATA_DIR, transform, 'test')\n",
    "train_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader   = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ---------- Optimizer & Loss ----------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()  # mixed precision\n",
    "\n",
    "train_losses, val_losses, val_accs = [], [], []\n",
    "start_epoch = 0\n",
    "\n",
    "# ---------- Load checkpoint if exists ----------\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    ckpt = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    if isinstance(ckpt, dict) and \"model_state_dict\" in ckpt:\n",
    "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "        start_epoch = ckpt.get(\"epoch\", 0) + 1\n",
    "        train_losses = ckpt.get(\"train_losses\", [])\n",
    "        val_losses = ckpt.get(\"val_losses\", [])\n",
    "        val_accs = ckpt.get(\"val_accs\", [])\n",
    "        print(f\" Resumed from epoch {start_epoch}\")\n",
    "    else:\n",
    "        model.load_state_dict(ckpt)\n",
    "        print(\" Loaded raw weights.\")\n",
    "\n",
    "# ---------- Training ----------\n",
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    loop = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for x, y in loop:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = output.argmax(1)\n",
    "        correct += preds.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "        loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    return total_loss / len(train_loader), 100 * correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    for x, y in tqdm(test_loader, desc=\"Evaluating\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss.item()\n",
    "        preds = out.argmax(1)\n",
    "        correct += preds.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    acc = 100 * correct / total\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    return avg_loss, acc, precision, recall, f1\n",
    "\n",
    "# ---------- Main Loop ----------\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "for epoch in range(start_epoch + 1, start_epoch + MAX_EPOCHS + 1):\n",
    "    start = time.time()\n",
    "    print(f\"\\n===== Epoch {epoch} =====\")\n",
    "    tr_loss, tr_acc = train_one_epoch()\n",
    "    val_loss, val_acc, prec, rec, f1 = evaluate()\n",
    "\n",
    "    train_losses.append(tr_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    elapsed = timedelta(seconds=int(time.time() - start))\n",
    "    print(f\" Train Loss: {tr_loss:.4f} | Train Acc: {tr_acc:.2f}%\")\n",
    "    print(f\" Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f}\")\n",
    "    print(f\" Duration: {elapsed}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    tmp = CHECKPOINT_PATH + \".tmp\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_accs\": val_accs\n",
    "    }, tmp)\n",
    "    os.replace(tmp, CHECKPOINT_PATH)\n",
    "    print(f\" Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "print(\"\\n Training completed successfully.\")\n",
    "\n",
    "# ---------- Plot ----------\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.legend(); plt.grid(True); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(val_accs, label=\"Val Accuracy\")\n",
    "plt.legend(); plt.grid(True); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_progress.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake4-env (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
